---
title: "Introduction to Machine Learning for Statisticians"
author: "Rafael A. Irizarry"
keywords: "Machine Learning"
date: "2024-12-02"
format:
  revealjs:
    theme: night
    mathjax: true
execute:
  echo: true
  cache: true
  fig-align: center
---

```{r setup, include=FALSE}
options(digits = 3)
```

## Learning objectives

- Motivation - zip code reader

- Terminology and statistical framing

- Resampling: cross validation and bootstrapping

- Quick overview of kNN, LDA, QDA, CART, Random Forests, Ensembles

- Hands on with **caret** package

- More details, code examples, and exercises in [this dsbook](https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/intro-ml.html)

## What we won't cover

- Details of algorithms

- Deep learning

- **scikit-learn**

- TensorFlow / Keras

- PyTorch

## Packages

```{r}
#| eval: false

install.packages("dslabs")
install.packages("caret")
install.packages("tidyverse")
install.packages("MASS")
install.packages("randomForest")
install.packages("matrixStats")
install.packages("doParallel")
install.packages("scales")
install.packages("ggrepel")
install.packages("gridExtra")

## optional
install.packages("knitr")
install.packages("rafalib")
```


## Motivation

- The MNIST dataset was generated by digitizing thousands of handwritten digits, already read and annotated by [humans](http://yann.lecun.com/exdb/mnist/)

- In the next slide we see three images of written digits.


## Motivation 


```{r digit-images-example, echo=FALSE, cache=FALSE, message=FALSE}
library(tidyverse)
library(dslabs)
ds_theme_set()
if (!exists("mnist")) mnist <- read_mnist()
tmp <- lapply( c(1,4,5), function(i){
    expand.grid(Row = 1:28, Column = 1:28) |>  
      mutate(id = i, label = mnist$train$label[i],  
             value = unlist(mnist$train$images[i,])) 
})
tmp <- Reduce(rbind, tmp)
tmp |> ggplot(aes(Row, Column, fill = value)) + 
    geom_raster(show.legend = FALSE) + 
    scale_y_reverse() +
    scale_fill_gradient(low = "white", high = "black") +
    facet_grid(.~label)
```

## Motivation 

- The images are converted into $28 \times 28 = 784$ pixels 

- for each pixel we obtain a grey scale intensity between 0 (white) and 255 (black). 

- Next slide shows the individual variables for each image.

## Motivation 

```{r example-images, echo=FALSE}
library(ggplot2)
tmp |> ggplot(aes(Row, Column, fill = value)) + 
    geom_point(pch = 21) + 
    scale_y_reverse() +
    scale_fill_gradient(low = "white", high = "black") +
    facet_grid(.~label)
```



## Motivation 

Read in data with **dslab** function `read_mnist`

```{r}
library(dslabs)
if (!exists("mnist")) mnist <- read_mnist()
```

The pixel intensities are saved in a matrix:

```{r}
class(mnist$train$images)
```

The labels associated with each image are in a vector:

```{r}
table(mnist$train$labels)
```

## Motivation 

:::::: {.center .vcenter}
By the end of this lecture you will be able to build a model that predict the digit using the image with over 95% accuracy.
:::

## Terminology

- **Outcome** - The variable we want to predict.
- **Features** - The variables we use to make the prediction; also called **covariates** or **predictors**.
- **Algorithm** - An approach that takes feature values as input and output predicts  the outcome.
- **Training** - We *train* an algorithm using a dataset for which the outcomes are known, then apply the model to new data where the outcomes are unknown.
- **Model** - The trained version of an algorithm after it has learned from data.

## Terminology 

- Prediction problems can be divided into:
  - _categorical_ outcomes.
  - _continuous_ outcomes.

## Categorical

- The number of classes can vary greatly across applications.

- We denote the $K$ categories with indexes $k=1,\dots,K$.

- However, for binary data we will use $k=0,1$ for mathematical conveniences that we demonstrate later.

- Examples: medical diagnostic tools, spam detectors, zip coder readers. 

## Continuous

- Examples: stock prices, real estate prices, temperature next week, student performance


## Notation

- We use $y_i$ to denote the i-th outcome 

- $x_{i,1}, \dots, x_{i,p}$ the corresponding features.

- We use matrix notation $\mathbf{x}_i = (x_{i,1}, \dots, x_{i,p})^\top$ to denote the vector of predictors.


## Notation

- Because, we often use statistical models to motivate algorithms we also use capital letters:

$$
Y \mbox{ and } \mathbf{X} = (X_{1}, \dots, X_{p})
$$

- We drop the index $i$ because it represents the **random variable** that generates observations.

- We use lower case, for example $\mathbf{X} = \mathbf{x}$, to denote observed values.

## Notation

- The machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features:

$$
\widehat{y}(\mathbf{x}) = f(x_1,x_2,\dots,x_p)
$$

- We will learn several approaches to building these algorithms.



## The machine learning challenge

- We have features and an unknown outcom:

:::{style="font-size: 70%;"}
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
library(tidyverse) 
library(knitr) 
library(dslabs) 
tmp <- tibble(outcome = "?",  
              'feature&nbsp;1' = "$X_1$", 
              'feature&nbsp;2' = "$X_2$", 
              'feature&nbsp;3' = "$X_3$", 
              '$\\dots$' = "$\\dots$", 
              'feature&nbsp;p' = "$X_p$") 
knitr::kable(tmp, "markdown") 
``` 
:::


## The machine learning challenge

- To *build a model* we collect data for which we know the outcome:

:::{style="font-size: 70%;"}
```{r, echo=FALSE} 
n <- 2 
tmp <- tibble(outcome = paste0("$y_{", 1:n,"}$"),  
              'feature&nbsp;1' = paste0("$x_{",1:n,",1}$"), 
              'feature&nbsp;2' = paste0("$x_{",1:n,",2}$"), 
              'feature&nbsp;3' = paste0("$x_{",1:n,",3}$"), 
              '$\\dots$' = paste0("$\\dots$"), 
              'feature&nbsp;p' = paste0("$x_{",1:n,",p}$")) 
tmp_2 <- rbind(c("$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\ddots$", "$\\vdots$"), 
               c("$y_n$", "$x_{n,1}$","$x_{n,2}$","$x_{n,3}$","$\\dots$","$x_{n,p}$")) 
colnames(tmp_2) <- names(tmp) 
tmp <- bind_rows(tmp, as_tibble(tmp_2)) 
knitr::kable(tmp, "markdown") 
``` 
:::


## Continuous data

- We refer to the ML task as *prediction*.

- We use the term *actual outcome* $y$ to denote what we end up observing.

- We want the prediction $\widehat{y}$ to match the actual outcome $y$ as best as possible.

- We define *error* as the difference between the prediction and the actual outcome $y - \widehat{y}$.


## Categorical data


- We refer to the ML task as *classification*

- Output of the model will be a *decision rule* which prescribes which of the $K$ classes tp predict.

- Provide a function $f_k(x_1, x_2, \dots, x_p)$, for each class $k$, and predict using:

$$
\hat{y}(x) = \arg\max_{k \in \{1,\ldots,K\}} f_k(x)
$$

- Here predictions will be either right or wrong.


::: {.center .vcenter}
# Evaluation metrics
:::



## Evaluation metrics


- We introduce the **caret** package, which provides useful functions to facilitate machine learning in R.

- For our first example, we use the height data provided by the **dslabs** package.


```{r, message=FALSE, warning=FALSE, cache=FALSE} 
library(dslabs) 
``` 

- We start by defining the outcome and predictors.

```{r} 
names(heights) <- c("y", "x")
``` 

## Training and test sets

```{r} 
library(caret) 
set.seed(2010)
train_index <- createDataPartition(heights$y, times = 1, p = 0.5, list = FALSE)
``` 


- We can use the result of the `createDataPartition` function call to define the training and test sets as follows:

```{r} 
train_set <- heights[train_index, ]
test_set <- heights[-train_index, ]
```

## Overall accuracy

- Let's start by developing the simplest possible machine algorithm: guessing the outcome.

```{r} 
y_hat <- factor(sample(c("Female", "Male"), nrow(test_set), replace = TRUE))
``` 

- The _overall accuracy_ is simply defined as the overall proportion that is predicted correctly:

```{r} 
mean(y_hat == test_set$y) 
``` 

## Overall accuracy

- Can we do better?

- Exploratory data analysis suggests we can because, on average, males are slightly taller than females:

```{r, warning=FALSE, message=FALSE} 
cutoff <- with(train_set, mean(x) - 2*sd(x))
y_hat <- factor(ifelse(test_set$x <= cutoff, "Female", "Male"))
``` 

- The accuracy goes up from 0.50 to about 0.80:

```{r} 
mean(test_set$y == y_hat) 
``` 

## Overall accuracy

- But can we do even better?

- Here we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:

```{r}
cutoffs <- seq(61, 70)
accuracy <- sapply(cutoffs, function(cutoff){
  y_hat <- factor(ifelse(train_set$x <= cutoff, "Female", "Male"))
  mean(y_hat == train_set$y)
})
```

## Overall accuracy

We can make a plot showing the accuracy obtained on the training set for males and females:

```{r accuracy-vs-cutoff, echo=FALSE}
library(ggplot2)
data.frame(cutoff = cutoffs, accuracy) |> 
  ggplot(aes(cutoff, accuracy)) + 
  geom_point() + 
  geom_line() 
```

## Overall accuracy

We see that the maximum value is:

```{r}
max(accuracy)
```

The cutoff resulting in this accuracy is:

```{r}
best_cutoff <- cutoffs[which.max(accuracy)]
best_cutoff
```

On the test set:

```{r}
y_hat <- factor(ifelse(test_set$x <= best_cutoff, "Female", "Male"))
mean(y_hat == test_set$y)
```


## The confusion matrix

- Given that the average female is about `r best_cutoff` inches, this prediction rule seems wrong. What happened? 

```{r}
cm <- confusionMatrix(data = y_hat, reference = test_set$y)
cm$table
```

## The confusion matrix

If we study this table closely, it reveals a problem:

```{r}
cm$byClass[c("Sensitivity", "Specificity")]
```

## The confusion matrix

The imbalance is due to:

```{r}
cm$byClass["Prevalence"]
```


## Reminder

:::{style="font-size: 80%;"}
| Measure&nbsp;of | Name&nbsp;1 | Name&nbsp;2 | Definition | Probability representation |
|---------|-----|----------|--------|------------------|
sensitivity | TPR | Recall | $\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}$ | $\mathrm{Pr}(\widehat{Y}=1 \mid Y=1)$ |
specificity | TNR | 1-FPR | $\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}$ | $\mathrm{Pr}(\widehat{Y}=0 \mid Y=0)$ |
specificity |  PPV | Precision | $\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}$ | $\mathrm{Pr}(Y=1 \mid \widehat{Y}=1)$|
:::

## The confusion matrix

- The __caret__ function `confusionMatrix` computes all these metrics for us once we define which category is the positive ($Y=1$). 

- The function expects factors as input, and the first level is considered the positive outcome, though it can be redefined with the `positive` argument. 

If you type this into R:

```{r}
#| eval: false
cm <- confusionMatrix(data = y_hat, reference = test_set$sex)
print(cm)
```


## Balanced accuracy and $F_1$ score


$$
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} + 
    \frac{1}{\mbox{precision}}\right) }
= 2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
$$


## Balanced accuracy and $F_1$ score

$$
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} + 
    \frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
$$


## Balanced accuracy and $F_1$ score

Let's rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:

```{r}
cutoffs <- seq(61, 70)
F_1 <- sapply(cutoffs, function(cutoff){
  y_hat <- factor(ifelse(train_set$x <= cutoff, "Female", "Male"))
  F_meas(y_hat, train_set$y)
})
```

## Balanced accuracy and $F_1$ score

We see that it is maximized at $F_1$ value of:

```{r}
max(F_1)
```

This maximum is achieved when we use the following cutoff:

```{r}
best_cutoff <- cutoffs[which.max(F_1)]
best_cutoff
```

## Balanced accuracy and $F_1$ score

which gives more sensible results:

```{r}
y_hat <- factor(ifelse(test_set$x <= best_cutoff, "Female", "Male"))
sensitivity(y_hat, test_set$y)
specificity(y_hat, test_set$y)
```


## ROC and precision-recall curves


```{r roc-1, echo=FALSE}
probs <- seq(0, 1, length.out = 10)
guessing <- sapply(probs, function(p){
  y_hat <- sample(c("Male", "Female"), nrow(test_set), TRUE, c(p, 1 - p)) 
  y_hat <- factor(y_hat, levels = c("Female", "Male"))
  c(FPR = 1 - specificity(y_hat, test_set$y),
    TPR = sensitivity(y_hat, test_set$y))
})
```


```{r, echo=FALSE}
cutoffs <- c(50, seq(60, 75), 80)
height_cutoff <- sapply(cutoffs, function(x){
  y_hat <- ifelse(test_set$x <= x, "Female", "Male")
  y_hat <- factor(y_hat, levels = c("Female", "Male"))
  c(FPR = 1 - specificity(y_hat, test_set$y),
    TPR = sensitivity(y_hat, test_set$y))
})
```

```{r roc-3, warning=FALSE, message=FALSE, echo=FALSE, fig.width=6, fig.height=3}
library(tidyverse)
library(ggrepel)
tmp_1 <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$x <= x, "Female", "Male") 
  y_hat <- factor(y_hat, levels = c("Female", "Male"))
  list(method = "Height cutoff",
       cutoff = x, 
       FPR = 1 - specificity(y_hat, test_set$y),
       TPR = sensitivity(y_hat, test_set$y))
}) 
tmp_2 <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), nrow(test_set), replace = TRUE, prob = c(p, 1 - p))
  y_hat <- factor(y_hat, levels = c("Female", "Male"))
  list(method = "Guessing",
       cutoff = round(p,1),
       FPR = 1 - specificity(y_hat, test_set$y),
       TPR = sensitivity(y_hat, test_set$y))
})

rbind(tmp_1, tmp_2) |>
  ggplot(aes(FPR, TPR, label = cutoff, color = method)) +
  geom_line() +
  geom_point() +
  geom_text_repel(nudge_x = 0.01, nudge_y = -0.01, show.legend = FALSE)
```


## Precision recall

```{r precision-recall-1, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
guessing <- map_df(probs[-1], function(p){
  y_hat <- sample(c("Male", "Female"), nrow(test_set), replace = TRUE, prob = c(p, 1 - p)) 
  y_hat <- factor(y_hat, levels = c("Female", "Male"))
  list(method = "Guess",
    recall = sensitivity(y_hat, test_set$y),
    precision = precision(y_hat, test_set$y))
})

height_cutoff <- map_df(cutoffs[-1], function(x){
  y_hat <- ifelse(test_set$x <= x, "Female", "Male")
  y_hat <- factor(y_hat, levels = c("Female", "Male"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, test_set$y),
    precision = precision(y_hat, test_set$y))
})
tmp_1 <- bind_rows(guessing, height_cutoff) |> mutate(Positive = "Y = 1 if Female") 

test_set2 <- test_set
test_set2$y <- relevel(test_set2$y, ref = "Male")
guessing <- map_df(probs[-1], function(p){
  y_hat <- sample(c("Male", "Female"), nrow(test_set2), replace = TRUE, prob = c(p, 1 - p)) 
  y_hat <- factor(y_hat, levels = c("Male", "Female"))
  list(method = "Guess",
    recall = sensitivity(y_hat, test_set2$y),
    precision = precision(y_hat, test_set2$y))
})

height_cutoff <- map_df(cutoffs[-1], function(x){
  y_hat <- ifelse(test_set2$x <= x, "Female", "Male")
  y_hat <- factor(y_hat, levels = c("Male", "Female"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, test_set2$y),
    precision = precision(y_hat, test_set2$y))
})
tmp_2 <- bind_rows(guessing, height_cutoff) |> mutate(Positive = "Y = 1 if Male") 


bind_rows(tmp_1, tmp_2) |>
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  ylim(0,1) +
  geom_point() +
  facet_wrap(~ Positive)
```


## Prevalence reminder

If specificity and sensitivity are high you can still get low precision.
$$
\mathrm{Pr}(Y = 1\mid \widehat{Y}=1) = \mathrm{Pr}(\widehat{Y}=1 \mid Y=1) \frac{\mathrm{Pr}(Y=1)}{\mathrm{Pr}(\widehat{Y}=1)} 
$$

Example: specificity and sensitivity is 0.99 but prevalence is 0.005:

$$
\text{Precision} = \text{TPR} \frac{\pi}{\text{TPR}\cdot\pi + \text{FPR}\cdot(1-\pi)} \approx 0.33  
$$


## Prevalence reminder

Here is plot of precision as a function of prevalence with TPR and TNR both equal to 99%:

```{r precision-vs-prevalence, echo=FALSE}
tpr <- 0.99; fpr <- 0.01
prevalence <- seq(0,1,len = 100)
data.frame(Prevalence = prevalence,
           Precision = tpr*prevalence/(tpr*prevalence + fpr*(1 - prevalence))) |>
  ggplot(aes(Prevalence, Precision)) + geom_line() + 
  labs(title = "Precision as a function of prevalence", subtitle = "when TPR = 0.99 and TNR = 0.99")
```


## Mean Squared Error

MSE is defined as:

$$
\text{MSE} \equiv \mathrm{E}[(\widehat{Y} - Y)^2 ]
$$


If the outcomes are binary, the MSE is equivalent to one minus expected accuracy, since $(\widehat{y} - y)^2$ is 0 if the prediction was correct and 1 otherwise. 

## Mean Squared Error 

- MSE is a theoretical quantity that depends on the unknown data-generating process. 

- The *apparent error* is a simple estimate:

$$
\widehat{\mbox{MSE}} = \frac{1}{N}\sum_{i=1}^N (\widehat{y}_i - y_i)^2
$$

with the $\widehat{y}_i$ generated completely independently from the the $y_i$s. 

- Important to remember: $\widehat{\text{MSE}}$ is a random variable. 

## Root Mean Squared Error 

In practice, we often report the root mean squared error (RMSE), which is simply $\sqrt{\mbox{MSE}}$, because it is in the same units as the outcomes.

::: {.center .vcenter}
# Resampling Methods
:::


## Is it a 2 or a 7? 

- For motivation purposes we will construct a simplified version of the MNIST dataset
- Can we tell if a digit is a 2 or 7 from the proportion of dark pixels in the upper left quadrant ($X_1$) and the lower right quadrant ($X_2$)?

## Is it a 2 or a 7? 

- Here are some digits with extreme values of $X_1$ or $X_2$

```{r two-or-seven-images-large-x1, echo=FALSE, fig.height=6, fig.width=11}
library(dslabs)
if (!exists("mnist")) mnist <- read_mnist()
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1), which.max(mnist_27$train$x_1))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row = 1:28, Column = 1:28) |>  
      mutate(label = titles[i],  
             value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
p1 <- tmp |> ggplot(aes(Row, Column, fill = value)) + 
  geom_raster(show.legend = FALSE) + 
  scale_y_reverse() +
  scale_fill_gradient(low = "white", high = "black") +
  facet_grid(.~label) + 
  geom_vline(xintercept = 14.5) +
  geom_hline(yintercept = 14.5) +
  ggtitle("Largest and smallest x_1")

    
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2), which.max(mnist_27$train$x_2))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row = 1:28, Column = 1:28) |>  
      mutate(label = titles[i],  
             value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
p2 <- tmp |> ggplot(aes(Row, Column, fill = value)) + 
    geom_raster(show.legend = FALSE) + 
    scale_y_reverse() +
    scale_fill_gradient(low = "white", high = "black") +
    facet_grid(.~label) + 
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5) +
  ggtitle("Largest and smallest x_2")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

## Is it a 2 or a 7? 

- The `mnist_27` dataset has:


* an outcome $y_i$: indicating whether the digit is a 2 or a 7, and
* a feature vector $\mathbf{x}_i = (x_{i,1}, x_{i,2})^\top$: a point in two-dimensional space extracted from the image.

## Is it a 2 or a 7? 


```{r two-or-seven-scatter, warning=FALSE, message=FALSE, cache=FALSE}
library(caret)
library(dslabs)
mnist_27$train |> ggplot(aes(x_1, x_2, color = y)) + geom_point()
```

## Is it a 2 or a 7? 


This is an example for teaching: We know the true $p(\mathbf{x})$

```{r true-p-better-colors, echo=FALSE, fig.asp=0.9, out.width="50%"}
mnist_27$true_p |> 
  ggplot(aes(x_1, x_2, z = p)) +
  geom_raster(aes(fill = p)) +
  scale_fill_gradientn(colors = c("#F8766D", "white", "#00BFC4")) +
  stat_contour(breaks = c(0.5), color = "black")
```


## Logistic regression

- Let's try building an algorithm using a GLM. 

$$
\log\frac{p(\mathbf{x})}{1-p(\mathbf{x})} = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$


- We fit can fit this model to obtain an estimate $\widehat{p}(\mathbf{x})$ by using the `glm` function.

- We define a decision rule by predicting $\widehat{y}(\mathbf{x})=1$ if $\widehat{p}(\mathbf{x})>0.5$ and 0 otherwise. 


## Logistic regression

```{r, echo=FALSE}
fit <- glm(y ~ x_1 + x_2, data = mnist_27$train, family = binomial)
p_hat <- predict(fit, newdata = mnist_27$test, type = "response")
y_hat <- factor(ifelse(p_hat > 0.5, 7, 2))
```

We get an accuracy of

```{r}
mean(y_hat == mnist_27$test$y)
```



## Logistic regression

We know logistic regression fails because boundary is:

$$
\widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \widehat{\beta}_2 x_2 = 0 \implies
x_2 = -\widehat{\beta}_0/\widehat{\beta}_2  -\widehat{\beta}_1/\widehat{\beta}_2 x_1
$$

which implies $x_2$ must be a linear function of $x_1$. 

## Logistic regression

GLM approach has no chance of capturing the non-linear boundary:

```{r regression-p-hat, echo=FALSE, fig.asp = 0.9, out.width="50%"}
p_hat <- predict(fit, newdata = mnist_27$true_p, type = "response")
mnist_27$true_p |> mutate(p_hat = p_hat) |>
  ggplot(aes(x_1, x_2,  z = p_hat)) +
  geom_raster(aes(fill = p_hat)) +
  scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks = c(0.5), color = "black") 
```

## k-nearest neighbors 

- We are interested in estimating the conditional probability function: $p(\mathbf{x})$


- k-nearest neighbors (kNN) we estimate in a similar way to bin smoothing. 

- First, we define the distance between all observations based on the features. 

- Then, for any point $\mathbf{x}_0$, we estimate $p(\mathbf{x}_0)$ by identifying the $k$ nearest points to $\mathbf{x}_0$ and taking an average of the $y$s associated with these points. 


## k-nearest neighbors 

```{r}
library(caret)
knn_fit <- knn3(y ~ ., data = mnist_27$train, k = 5)
```


The `predict` function for `knn3` returns estimates of $p(\mathbf{x})$ (`type = "prob"`) or the prediction (`type = "class"`):

```{r}
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
```


## k-nearest neighbors 

We see that kNN, with the default parameter, already beats GLM:

```{r}
mean(y_hat_knn == mnist_27$test$y)
```

## k-nearest neighbors 

To see why this is the case, we plot $\widehat{p}(\mathbf{x})$ and compare it to the true conditional probability $p(\mathbf{x})$:

```{r, echo = FALSE, message=FALSE, warning=FALSE, cache=FALSE}
# We use this function to plot the estimated conditional probabilities
plot_cond_prob <- function(p_hat = NULL){
  tmp <- mnist_27$true_p
  if (!is.null(p_hat)) {
    tmp <- mutate(tmp, p = p_hat)
  }
  tmp |> ggplot(aes(x_1, x_2, z = p)) +
  geom_raster(aes(fill = p), show.legend = FALSE) +
  scale_fill_gradientn(colors = c("#F8766D", "white", "#00BFC4")) +
  stat_contour(breaks = c(0.5), color = "black")
}
```

```{r knn-fit, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 0.5}
library(tidyverse)
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) +
  ggtitle("kNN-5 estimate")
library(gridExtra)

grid.arrange(p2, p1, nrow = 1)
```

## k-nearest neighbors 

However, we notice that we have higher accuracy in the train set compared to the test set:

```{r}
y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
mean(y_hat_knn == mnist_27$train$y)

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
mean(y_hat_knn == mnist_27$test$y)
```

This is due to what we call *over-training*.

## Over-training

Here we fit a kNN model with $k = 1$ and confirm we get near perfect accuracy in the training set:

```{r}
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class")
mean(y_hat_knn_1 == mnist_27$train$y)
```

But in the test set, accuracy is actually worse than what we obtained with regression:

```{r}
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$test, type = "class")
mean(y_hat_knn_1 == mnist_27$test$y)
```

## Over-training

We can see the over-fitting problem by plotting  the decision rule boundaries produced by $\widehat{p}(\mathbf{x})$:


```{r knn-1-overfit, echo = FALSE, fig.asp = 0.5}
tmp <- mnist_27$true_p
tmp$knn <- predict(knn_fit_1, newdata = mnist_27$true_p)[,2]
p1 <- tmp |>
  ggplot() +
  geom_point(data = mnist_27$train, aes(x_1, x_2, color = y),
             pch = 21, show.legend = FALSE) +
  scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) +
  stat_contour(aes(x_1, x_2, z = knn), breaks = c(0.5), color = "black") +
  ggtitle("Train set")

p2 <- tmp |>
  ggplot() +
  geom_point(data = mnist_27$test, aes(x_1, x_2, color = y), 
             pch = 21, show.legend = FALSE) +
  scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) +
  stat_contour(aes(x_1, x_2, z = knn), breaks = c(0.5), color = "black") +
  ggtitle("Test set")

grid.arrange(p1, p2, nrow = 1)
```

## Over-smoothing

Although not as badly as with $k=1$, we saw that with $k = 5$ we also over-trained. Hence, we should consider a larger $k$. Let's try, as an example, a much larger number: $k = 401$.

```{r}
knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class")
mean(y_hat_knn_401 == mnist_27$test$y)
```

## Over-smoothing

The estimate turns out to be similar to the one obtained with regression:

```{r mnist-27-glm-est, echo = FALSE, fig.asp = 0.5}
fit_lm <- lm(y ~ ., data = mutate(mnist_27$train, y=y == "7"))
p_hat <- predict(fit_lm, newdata = mnist_27$true_p)
p_hat <- scales::squish(p_hat, c(0, 1))
p1 <- plot_cond_prob(p_hat) +
  ggtitle("Regression")

p2 <- plot_cond_prob(predict(knn_fit_401, mnist_27$true_p)[,2]) +
  ggtitle("kNN-401")
  
grid.arrange(p1, p2, nrow = 1)
```


## Tuning parameters

- It is very common for machine learning algorithms to require that we set one or more values *before* fitting the model. 

- These values are referred to as *tuning parameters*, 

- an important part of applying machine learning in practice is choosing them, often called *tuning the model*.

- So how do we pick tuning parameters? 


## Tuning parameters

```{r, echo=FALSE,warning = FALSE, message = FALSE}
ks <- seq(3, 251, 2)
accuracy <- sapply(ks, function(k){
  fit <- knn3(y ~ ., data = mnist_27$train, k = k)
  
  y_hat <- predict(fit, mnist_27$train, type = "class")
  cm_train <- confusionMatrix(y_hat, mnist_27$train$y)
  train_error <- cm_train$overall[["Accuracy"]]
  
  y_hat <- predict(fit, mnist_27$test, type = "class")
  cm_test <- confusionMatrix(y_hat, mnist_27$test$y)
  test_error <- cm_test$overall[["Accuracy"]]
  
  c(train = train_error, test = test_error)
})
```

```{r accuracy-vs-k-knn, echo = FALSE}
data.frame(k = ks, train = accuracy["train",], test = accuracy["test",]) |>
  pivot_longer(-k, values_to = "accuracy", names_to = "set") |>
  mutate(set = factor(set, levels = c("test", "train"))) |>
  ggplot(aes(k, accuracy, color = set)) + 
  geom_line() +
  geom_point() 
```


## Tuning parameters

Should we simply pick the value of (k) with the highest test accuracy?
Two problems arise:

1. **The accuracy–vs–(k) curve is noisy.**
   Small changes in (k) shouldn’t cause big accuracy swings, but test accuracy jumps around because it’s based on a limited sample. The “best” (k) may look best just by chance.

2. **We reuse the test set.**
   We use it to *choose* (k) and then again to *report* accuracy, leading to an overly optimistic estimate that won’t generalize.

## Resampling methods


*Resampling methods* provide a principled solution to both problems by reducing variability and ensuring that test data are not used twice, once for evaluation and again for tuning.

## Resampling methods

- An intuitive first attempt is the apparent error:

$$
\widehat{\mbox{MSE}}(\lambda) = \frac{1}{N}\sum_{i = 1}^N \left\{\widehat{y}_i(\lambda) - y_i\right\}^2
$$

- As noted this estimate is a random variable, based on just one test set, with enough variability to affect the choice of the best $\lambda$ substantially.

## Resampling methods

- Imagine  we could obtain data repeatedly, 

- We take a very large number $B$ of new samples, split each of them into training and test sets, and define: 

$$
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left\{\widehat{y}_i^b(\lambda) - y_i^b\right\}^2
$$

- with $y_i^b$ the $i$th observation in test sample $b$ and $\widehat{y}_{i}^b(\lambda)$ the prediction obtained with the algorithm defined by parameter $\lambda$ and trained on train set $b$. 


## Resampling methods

- The law of large numbers tells us that as $B$ becomes larger, this quantity gets closer and closer to $\mbox{MSE}(\lambda)$.

- This is of course a theoretical consideration as we rarely get access to more than one dataset for algorithm development, but the concept inspires resampling methods.

- The general idea behind resampling methods is to generate a series of different random samples from the data at hand. 

## Cross validation 

```{r, include = FALSE}
if (knitr::is_html_output()) {
  knitr::opts_chunk$set(out.width = "500px",
                        out.extra = 'style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"')
} else{
  knitr::opts_chunk$set(out.width = "35%")
}
```


```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-1.png")
```

## Cross validation 

```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-3.png")
```

## Cross validation 

```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-4.png")
```


## K-fold cross validation

- As a reminder, we are going to imitate the concept used when introducing this version of the MSE:

$$
\mbox{MSE}(\lambda) \approx\frac{1}{B} \sum_{b = 1}^B \frac{1}{N}\sum_{i = 1}^N \left(\widehat{y}_i^b(\lambda) - y_i^b\right)^2 
$$


- We want to generate a dataset that can be thought of as independent random sample, and do this $B$ times. 

- The K in K-fold cross validation, represents the number of time $B$. 


## K-fold cross validation

- We  end up with $B$ samples,

- We simply pick $M = N/B$ observations at random  and think of these as a random sample $y_1^b, \dots, y_M^b$, with $b = 1$. 

- We call this the validation set.

- Now we can fit the model in the training set, then compute the apparent error on the independent set:

$$
\widehat{\mbox{MSE}}_b(\lambda) = \frac{1}{M}\sum_{i = 1}^M \left(\widehat{y}_i^b(\lambda) - y_i^b\right)^2 
$$

## K-fold cross validation

```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-5.png")
```


## K-fold cross validation

Now we repeat the calculation above for each of these sets $b = 1,\dots,B$ and obtain $\widehat{\mbox{MSE}}_1(\lambda),\dots, \widehat{\mbox{MSE}}_B(\lambda)$. Then, for our final estimate, we compute the average:

$$
\widehat{\mbox{MSE}}(\lambda) = \frac{1}{B} \sum_{b = 1}^B \widehat{\mbox{MSE}}_b(\lambda)
$$

and obtain an estimate of our loss. A final step would be to select the $\lambda$ that minimizes the MSE.

## How many folds?

- Large values of $B$ are preferable because the training data better imitates the original dataset. 

- However, larger values of $B$ will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. 


- For this reason, the choices of $B = 5$ and $B = 10$ are popular.


## Estimate optimized model MSE 


```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-6.png")
```


## Estimate optimized model MSE 


```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-7.png")
```

## Final model

Once we are satisfied with this model and want to make it available to others: 


```{r, echo = FALSE}
knitr::include_graphics("https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/img/cv-8.png")
```


```{r, include = FALSE}
knitr::opts_chunk$set(out.width = "70%", out.extra = NULL)
```


## Bootstrap resampling

* Generate many bootstrap samples by sampling the training data **with replacement**.
* For each sample, **fit the model** and compute MSE on the **out-of-bag** observations (those not selected).
* Out-of-bag data acts like a **built-in validation set**.
* **Average** the out-of-bag MSEs to estimate model performance.

## Comparison of MSE estimates 

```{r k-fold-versus-bootstrap, echo=FALSE, cache=TRUE}
set.seed(2023-11-30)
boot <- train(y ~ ., method = "knn", tuneGrid = data.frame(k=ks), 
              data = mnist_27$train, 
              trControl = trainControl(number = 100))
cv <- train(y ~ ., method = "knn", 
            tuneGrid = data.frame(k = ks), 
            data = mnist_27$train,
            trControl = trainControl(method = "cv", 
                                     number = 10, p = .9))

data.frame(k = ks, naive = accuracy["test",], 
           cv = cv$results[,2],
           boot = boot$results[,2]) |>
  pivot_longer(-k, values_to = "Accuracy", names_to = "set") |>
  mutate(set = factor(set, levels = c("naive", "cv", "boot"),
                      labels = c("Simple", "10-fold", "Boostrap"))) |>
  ggplot(aes(k, Accuracy, color = set)) + 
  geom_line() 
```


::: {.center .vcenter}
# Supervised Learning Methods
:::

## k-nearest neighbors

```{r, echo = FALSE, warning = FALSE, message = FALSE}
set.seed(2008)
library(tidyverse)
library(gridExtra)
library(dslabs)
library(caret)
# We use this function to plot the estimated conditional probabilities
plot_cond_prob <- function(p_hat = NULL){
  tmp <- mnist_27$true_p
  if (!is.null(p_hat)) {
    tmp <- mutate(tmp, p = p_hat)
  }
  tmp |> ggplot(aes(x_1, x_2, z = p)) +
  geom_raster(aes(fill = p), show.legend = FALSE) +
  scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks = c(0.5),color = "black")
}
```

```{r, echo=FALSE}
train_knn <- knn3(y ~ ., k = 65, data = mnist_27$train)
```


```{r best-knn-fit, echo = FALSE, fig.asp = 0.5}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_knn, newdata = mnist_27$true_p, type = "prob")[,2]) +
  ggtitle("kNN")

grid.arrange(p2, p1, nrow = 1)
```

## Probabilistic classification models


$$
\mathrm{Pr}(Y = 1|\mathbf{X}=\mathbf{x}) = \frac{f_{\mathbf{X}|Y = 1}(\mathbf{x}) \mathrm{Pr}(Y = 1)}
{ f_{\mathbf{X}|Y = 0}(\mathbf{x})\mathrm{Pr}(Y = 0)  + f_{\mathbf{X}|Y = 1}(\mathbf{x})\mathrm{Pr}(Y = 1) }
$$


## QDA

```{r}
#| echo: false
param <- mnist_27$train |> 
  group_by(y) |> 
  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), 
            sd_1= sd(x_1), sd_2 = sd(x_2), 
            r = cor(x_1, x_2))
```


```{r qda-explained, echo=FALSE}
mnist_27$train |> mutate(y = factor(y)) |> 
  ggplot(aes(x_1, x_2, fill = y, color = y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type = "norm", lwd = 1.5)
```

## QDA


```{r qda-estimate, echo = FALSE, fig.asp = 0.5, warning = FALSE, message = FALSE}
train_qda <- MASS::qda(y ~ ., data = mnist_27$train)
y_hat <- predict(train_qda, mnist_27$test)$class
library(gridExtra)
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_qda, newdata = mnist_27$true_p, type = "prob")$posterior[,2]) +
  ggtitle("QDA")

grid.arrange(p2, p1, nrow = 1)
```


## QDA

```{r qda-does-not-fit, fig.asp = 0.5, echo=FALSE}
mnist_27$train |> mutate(y = factor(y)) |> 
  ggplot(aes(x_1, x_2, fill = y, color = y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type = "norm") +
  facet_wrap(~y)
```

## LDA


```{r lda-explained, echo = FALSE}
param <- mnist_27$train |> 
  group_by(y) |> 
  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), 
            sd_1= sd(x_1), sd_2 = sd(x_2), 
            r = cor(x_1,x_2))

param <- param |> mutate(sd_1 = mean(sd_1), sd_2 = mean(sd_2), r = mean(r))

tmp <- lapply(1:2, function(i){
  with(param[i,], MASS::mvrnorm(1000, mu = c(avg_1, avg_2), Sigma = matrix(c(sd_1^2, sd_1*sd_2*r, sd_1*sd_2*r, sd_2^2), 2, 2))) |>
    as.data.frame() |> 
    setNames(c("x_1", "x_2")) |> 
    mutate(y  = factor(c(2,7)[i]))
})
tmp <- do.call(rbind, tmp)
mnist_27$train |> mutate(y = factor(y)) |> 
  ggplot() + 
  geom_point(aes(x_1, x_2, color = y), show.legend = FALSE) + 
  stat_ellipse(aes(x_1, x_2, color = y), data = tmp, type = "norm", lwd = 1.5)
```

## LDA



```{r lda-estimate, echo = FALSE, fig.asp = 0.5}
train_lda <- MASS::lda(y ~ ., data = mnist_27$train)

p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_lda, newdata = mnist_27$true_p, type = "prob")$posterior[,2]) +
  ggtitle("LDA")

grid.arrange(p2, p1, nrow = 1)
```


## Extension to multiple classes

$$
p_k(\mathbf{x}) = \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) = 
\frac{f_{\mathbf{X}\mid Y=k}(\mathbf{x}),\Pr(Y=k)}{
\sum_{l=1}^K f_{\mathbf{X}\mid Y=l}(\mathbf{x})\Pr(Y=l)
},\\
\qquad k = 1,\dots,K.
$$


## CART

```{r, echo=FALSE}
train_rpart <- train(y ~ .,
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
                     data = mnist_27$train)
y_hat <- predict(train_rpart, mnist_27$test)
```

```{r rf-cond-prob, echo = FALSE, fig.asp=0.5, warning = FALSE, message = FALSE}
library(gridExtra)
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_rpart, newdata = mnist_27$true_p, type = "prob")[,2]) +
  ggtitle("Decision Tree")

grid.arrange(p2, p1, nrow = 1)
```

## Random forests

```{r}
#| include: false
#| cache: false

library(randomForest)
```

```{r mnits-27-rf-fit, echo=FALSE}
library(randomForest)
train_rf <- randomForest(y ~ ., data = mnist_27$train)
```

```{r cond-prob-rf, echo = FALSE, fig.asp = 0.5}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_rf, newdata = mnist_27$true_p, type = "prob")[,2]) +
  ggtitle("Random Forest")

grid.arrange(p2, p1, nrow = 1)
```

## Random forests (nodesize = 65)

```{r, echo=FALSE}
train_rf_2 <- randomForest(y ~ ., data = mnist_27$train, nodesize = 65)
```

```{r cond-prob-final-rf, echo = FALSE, fig.asp=0.5}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_rf_2, newdata = mnist_27$true_p, type = "prob")[,2]) +
  ggtitle("Random Forest")

grid.arrange(p2, p1, nrow = 1)
```


::: {.center .vcenter}
#  Building Machine Learning Models 
:::

## Case study

- We will use the MNIST dataset. 

- We can load this data using the following **dslabs** package function:

```{r, message=FALSE, warning=FALSE}
library(dslabs)
mnist <- read_mnist()
```

## Case study

The dataset includes two components:

```{r}
names(mnist)
```

Each of these components includes a matrix with features in the columns:

```{r}
dim(mnist$train$images)
```

and vector with the classes as integers:

```{r}
class(mnist$train$labels)
table(mnist$train$labels)
```

## Case study

- For illustration we look at a subset

```{r}
set.seed(1990)
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])
index <- sample(nrow(mnist$test$images), 1000)
x_test <- mnist$test$images[index,]
y_test <- factor(mnist$test$labels[index])
```

## Case study


- When fitting models to large datasets, we recommend using matrices instead of data frames, as matrix operations tend to be faster. 

- In the **caret** package, predictor matrices must have column names to track features accurately during prediction on the test set. 

- If the matrices lack column names, you can assign names based on their position:

```{r}
colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x)
```

## The caret package 

```{r}
#| cache: false
#| include: false  
library(caret)
```

- There are dozens of algorithms implemented in R. 

- However, they are distributed via different packages, developed by different authors, and often use different syntax. 

- The __caret__ package tries to consolidate these differences and provide consistency. 

- You can learn more in the [manual](https://topepo.github.io/caret/available-models.html)

## The `train` function 


```{r}
#| message: false
#| warning: false
library(caret)
train_glm <- train(y ~ ., method = "glm", data = mnist_27$train)
train_qda <- train(y ~ ., method = "qda", data = mnist_27$train)
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
```

Note we are using `mnist_27` for this example.

## The `predict` function


```{r}
fit <- glm(y ~ ., family = "binomial", data = mnist_27$train)
p_hat <- predict(fit, newdata = mnist_27$test)
y_hat <- factor(ifelse(p_hat > 0.5, 7, 2))
```


However, there are many other versions of `predict`


## The `predict` function

Caret consolidates them:

```{r}
y_hat_glm <- predict(train_glm, mnist_27$test, type = "raw")
y_hat_qda <- predict(train_qda, mnist_27$test, type = "raw")
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
```

## The `predict` function

Example: quickly compare the algorithms. For example, we can compare the accuracy like this:

```{r}
fits <- list(glm = train_glm, qda = train_qda, knn = train_knn)
sapply(fits, function(fit) 
  mean(predict(fit, mnist_27$test, type = "raw") == mnist_27$test$y))
```


## Resampling 

- When an algorithm includes a tuning parameter, `train` automatically uses a resampling method to estimate MSE and decide among a few default candidate values.

- To find out what parameter or parameters are optimized, you can read the **caret** [manual](http://topepo.github.io/caret/available-models.html) or study the output of: 

```{r, eval=FALSE}
modelLookup("knn")
```


## Resampling 

To obtain all the details of how **caret** implements kNN you can use:

```{r, eval=FALSE}
getModelInfo("knn")
```

## Resampling 

If we run it with default values: 

```{r}
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
```

- By default, the resampling is performed by taking 25 bootstrap samples, each comprised of 25% of the observations. 

## Resampling 

you can quickly see the results of the cross validation using the `ggplot` function. The argument `highlight` highlights the max:

```{r caret-highlight}
#| warning: false
ggplot(train_knn, highlight = TRUE)
```

## Resampling 

- For the `knn` method, the default is to try $k=5,7,9$. 

- We change this using the `tuneGrid` argument. 

- The grid of values must be supplied by a data frame with the parameter names as specified in the `modelLookup` output. 

- Here we present an example where we try out 38 values between 1 and 75.

- To do this with __caret__, we need to define a column named `k`, so we use this:  `data.frame(k = seq(11, 105, 2))`. 

## Resampling 

```{r train-knn-plot}
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(11, 105, 4)))
ggplot(train_knn, highlight = TRUE)
```

## Resampling 

To access the parameter that maximized the accuracy, you can use this:

```{r}
train_knn$bestTune
```

and the best performing model like this:

```{r}
#| eval: false
train_knn$finalModel
```

## Resampling 

- The function `predict` will use this best performing model. 

- Here is the accuracy of the best model when applied to the test set, which we have not yet used because the cross validation was done on the training set:

```{r}
mean(predict(train_knn, mnist_27$test, type = "raw") == mnist_27$test$y)
```

## Resampling 

- Bootstrapping is not always the best approach to resampling. 

- If we want to change our resampling method, we can use the `trainControl` function. 

- For example, the code below runs 10-fold cross validation:

```{r cv-10-fold-accuracy-estimate}
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(11, 105, 4)),
                   trControl = control)
```


## Preprocessing

- We often transform predictors before running the machine algorithm.

- We also remove predictors that are clearly not useful. 

- We call these steps *preprocessing*.


## Preprocessing

- Examples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation. 

- For example, we can run the `nearZeroVar` function from the **caret** package to see that several features do not vary much from observation to observation. 


## Preprocessing

We can see that there is a large number of features with close to 0 variability:

```{r pixel-sds, message=FALSE, warning=FALSE, cache=FALSE, echo = -1}
rafalib::mypar()
library(matrixStats)
hist(colSds(x), breaks = 256)
```

## Preprocessing


This is expected because there are parts of the image that rarely contain writing (dark pixels).

The **caret** packages includes a function that recommends features to be removed due to *near zero variance*:

```{r, message=FALSE, warning=FALSE}
nzv <- nearZeroVar(x)
```

## Preprocessing


- The **caret** package features the `preProcess` function, which allows users to establish a predefined set of preprocessing operations based on a training set. 

- This function is designed to apply these operations to new datasets without recalculating anything on the validation set, ensuring that all preprocessing steps are consistent and derived solely from the training data.

- Below is an example demonstrating how to remove predictors with near-zero variance and then center the remaining predictors:

```{r}
pp <- preProcess(x, method = c("nzv", "center"))
centered_subsetted_x_test <- predict(pp, newdata = x_test)
dim(centered_subsetted_x_test)
```

## Parallelization 

- During cross-validation or bootstrapping, the process of fitting models to different samples or using varying parameters can be performed independently. 

- Imagine you are fitting 100 models; if you had access to 100 computers, you could theoretically speed up the process by a factor of 100 by fitting each model on a separate computer and then aggregating the results. 

- In reality, most modern computers, including many personal computers, are equipped with multiple processors that allow for such parallel execution. 

- This method, known as parallelization, leverages these multiple processors to conduct several computational tasks simultaneously, significantly accelerating the model training process.


## Parallelization 

The **caret** package is designed to take advantage of parallel processing, but you need to explicitly tell R to run tasks in parallel. To do this, we use the **doParallel** package:

```{r}
#| message: false
#| warning: false
library(doParallel)
nc <- detectCores() - 1   # it is convention to leave 1 core for the OS
cl <- makeCluster(nc)
registerDoParallel(cl)
```


## Parallelization 

If you do use parallelization, once you are finished fitting your models, make sure to let R know you are done with the following lines of code:

```{r}
#| eval: false
stopCluster(cl)
stopImplicitCluster()
```


## k-nearest neighbors 

The first step is to optimize for $k$.

```{r mnist-knn-fit}
train_knn <- train(x, y, method = "knn", 
                   preProcess = "nzv",
                   trControl = trainControl("cv", number = 20, p = 0.95),
                   tuneGrid = data.frame(k = seq(1, 7, 2)))
```


## k-nearest neighbors 

Once we optimize our algorithm, the `predict` function defaults to using the best performing algorithm fit with the entire training data:

```{r}
y_hat_knn <- predict(train_knn, x_test, type = "raw")
```

We achieve relatively high accuracy:

```{r}
mean(y_hat_knn == y_test)
```

## Dimension reduction with PCA

- An alternative to removing low‐variance columns is to reduce the dimension of the feature matrix with PCA.

- For this dataset, a large fraction of the variability can be captured using only a small number PCs.

- A critical point when using PCA for prediction is that the PCA transformation must be learned only from the training set. 

- If we use the validation or test set to compute PCs or even to compute the means used for centering, we inadvertently leak information from those sets into the training process, leading to overtraining.


## Dimension reduction with PCA

Here is how we modify our earlier code to let caret perform PCA during preprocessing:

```{r}
train_knn_pca <- train(x, y, method = "knn", 
                       preProcess = c("nzv", "pca"),
                       trControl = trainControl("cv", number = 20, p = 0.95,
                                                preProcOptions = list(thresh = 0.9)),
                       tuneGrid = data.frame(k = seq(1, 7, 2)))
y_hat_knn_pca <- predict(train_knn_pca, x_test, type = "raw")
mean(y_hat_knn_pca == y_test)
```


## Random Forest

- With the random forest algorithm several parameters can be optimized, but the main one is `mtry`, the number of predictors that are randomly selected for each tree. 

- This is also the only tuning parameter that the **caret** function `train` permits when using the default implementation from the **randomForest** package.


## Random Forest

```{r, eval=FALSE}
library(randomForest)
train_rf <- train(x, y, method = "rf", 
                  preProcess = "nzv",
                  tuneGrid = data.frame(mtry = seq(5, 15)))
y_hat_rf <- predict(train_rf, x_test, type = "raw")
```

Now that we have optimized our algorithm, we are ready to fit our final model:

```{r, eval=FALSE}
y_hat_rf <- predict(train_rf, x_test, type = "raw")
```


```{r}
#| include: false
#| cache: false
library(randomForest)
```

```{r}
#| echo: false
#| message: false
## we hard wire to make book compilation faster.
## we ran previous code once to determine 9 was the best
library(randomForest)
nzv <- nearZeroVar(x)
fit_rf <- randomForest(x[, -nzv], y,  mtry = 9)
y_hat_rf <- predict(fit_rf, x_test[,-nzv])
```

As with kNN, we also achieve high accuracy:

```{r}
mean(y_hat_rf == y_test)
```



## Testing and improving computation time

- The default method for estimating accuracy used by the `train` function is to test prediction on 25 bootstrap samples. 

- This can result in long compute times. For example, if we are considering several values, say 10, of the tuning parameters, we will fit the algorithm 250 times. 

## Testing and improving computation time

- We can use the `system.time` function to estimate how long it takes to run the algorithm once and use to calculate time.

```{r}
nzv <- nearZeroVar(x)
system.time({fit_rf <- randomForest(x[, -nzv], y,  mtry = 9)})
```


## Variable importance

- A limitation of methods such as random forests is that they are hard to interpret.

- Variable importance gives you an idea how much each predictor influenced the model. 

- The following function compute the importance of each feature:

```{r}
imp <- varImp(fit_rf)
```

## Variable importance

We can see which features are being used most by plotting an image:

```{r importance-image, echo=-1, fig.width = 4, fig.height = 4, out.width="50%", echo=FALSE}
rafalib::mypar()
mat <- rep(0, ncol(x))
mat[-nzv] <- imp$Overall
image(matrix(mat, 28, 28))
```

## Diagnostics

- An important part of data analysis is visualizing results to determine why we are failing. 

- How we do this depends on the application. Below we show the images of digits for which we made an incorrect prediction. 

## Diagnostics

- Here are some errors for the random forest:

```{r rf-images,, echo=FALSE, out.width="100%", fig.width=6, fig.height=1.65}
p_max <- predict(fit_rf, x_test[,-nzv], type = "prob") 
p_max <- p_max / rowSums(p_max)
p_max <- apply(p_max, 1, max)

ind  <- which(y_hat_rf != y_test)
ind <- ind[order(p_max[ind], decreasing = TRUE)]

rafalib::mypar(1,4)
for (i in ind[1:4]) {
  image(matrix(x_test[i,], 28, 28)[, 28:1], 
        main = paste0("Pr(",y_hat_rf[i],")=",round(p_max[i], 2), " but is a ",y_test[i]),
        xaxt = "n", yaxt = "n")
}
```


## Ensembles

- The idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate.

- In machine learning, one can usually greatly improve the final results by combining the results of different algorithms.

## Ensembles

Here is a simple example where we compute new class probabilities by taking the average of random forest and kNN. We can see that the accuracy improves:

```{r}
p_rf <- predict(fit_rf, x_test[,-nzv], type = "prob")  
p_rf <- p_rf / rowSums(p_rf)
p_knn_pca  <- predict(train_knn_pca, x_test, type = "prob")
p <- (p_rf + p_knn_pca)/2
y_pred <- factor(apply(p, 1, which.max) - 1)
mean(y_pred == y_test)
```

We have just built an ensemble with just two algorithms. By combing more similarly performing, but uncorrelated, algorithms we can improve accuracy further.

```{r}
#| echo: false
stopCluster(cl)
stopImplicitCluster()
```
